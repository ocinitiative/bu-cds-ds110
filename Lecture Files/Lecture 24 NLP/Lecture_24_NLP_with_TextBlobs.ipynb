{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Intro to Natural Language Processing (NLP)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "(based on Ch 12 of Deitel and Deitel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NfDyuDIS84c5",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Tasks in natural language processing (NLP)\n",
    "\n",
    "What might we want to do with text in an automated way?  Understanding the full meaning of text is still beyond our capabilities, but we could still analyze which words or sequences of words are common in the text.  That is itself a stepping stone to...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "\n",
    "* Sentiment analysis - determining whether people are happy or unhappy. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "\n",
    "* Topic classification - Determining whether a document is relevant to a topic.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "\n",
    "* Named entity recognition - What proper nouns are being talked about, for example for sentiment analysis or stock prediction.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "\n",
    "* Translation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "\n",
    "* Exploratory visualization - a word cloud or visualization in a \"semantic space\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "\n",
    "* Chatbots - for helping or entertainment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MUGOXZRL-80Y",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# TextBlobs and their properties\n",
    "\n",
    "TextBlobs are sort of like strings with many bells and whistles attached to them.  The TextBlob module provides an easy-to-use interface on top of two powerful tools for NLP, the nltk module and the pattern module.  After a TextBlob is created, many of its features are usable by just accessing attributes of the TextBlob.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "\n",
    "Normally, breaking strings down into words might be done with the split() method, and we'd still have punctuation lying around.  A TextBlob immediately knows what sentences it has and what words it has, and the words aren't attached to their punctuation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "These features use the nltk tokenizer, and so the data used for that needs to be downloaded first.  (A tokenizer breaks a sentence into meaningful words and punctuation - tokens.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nMvtprFjAThR",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5oUe9cbN8soe",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "!pip install textblob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "from textblob import TextBlob\n",
    "text = TextBlob('Hello, textblob!  Hello, sentences and words!')\n",
    "\n",
    "print(text.sentences)\n",
    "print(text.words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JDYVSF8nAkpU",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The TextBlob is already doing some nontrivial tokenizing work for us, as detecting sentences and tokens has some subtle issues.  This work is often done by a lightweight machine learning algorithm, like a perceptron (a single unit neural network).  In the next example, a comma and a period are both correctly interpreted as not being sentence-level punctuation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TgftA3XUAwHT",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "text = TextBlob('I can\\'t decide whether I want to get $1,000 or give my $0.02.')\n",
    "print(text.sentences)\n",
    "print(text.words)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KjSFobZ8BaCl",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Part-of-speech (POS) tagging is another task that comes built in to every text blob; it also typically uses a perceptron or similar lightweight machine learning.  Just accessing the .tags field gives a list of tuples of words and parts of speech.  (Like the tokenizing, this borrows a trained classifier from nltk - a perceptron trained to predict part of speech from nearby words.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8JxZb7LeBsba",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "text.tags"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4zIYq-9hCnzK",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Umm, what?  Thankfully, nltk (which TextBlobs use) comes with a command to see what each of the abbreviations means."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Kr7Sz_k8CqLw",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "nltk.download('tagsets')\n",
    "nltk.help.upenn_tagset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7W6kFCObDQGp",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "TextBlobs even come with a ready-to-use \"sentiment\" field that uses more lightweight machine learning (averaging word sentiment scores) to determine the polarity of the sentiment (negative or positive, in range [-1,1]) and the subjectivity of the sentence (range [0,1] with 1 subjective).  The second number can tell you whether you ought to use this sentence for sentiment analysis at all, or how much you might weight it in a classification of the document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tELqEjMtEE52",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "happy_text = TextBlob(\"This is a pretty neat module.\")\n",
    "happy_text.sentiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EWWTfsroEsra",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "This doesn't work all the time (it allegedly gets 75% accuracy on movie reviews, and less on text that isn't reviews), and you'll probably get better performance out of a more powerful sentiment analysis technique.  But it is kind of cute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cwQ63GWAEgfa",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "unhappy_text = TextBlob(\"But, I don't like how I can't really tell how it makes its decisions.\")\n",
    "unhappy_text.sentiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dpQ_ryfzE_UL",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "There are a few different analyzers to try, including a Naive Bayes analyzer (see DS120) trained on movie reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Cz85APtVFFhR",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "from textblob.sentiments import NaiveBayesAnalyzer\n",
    "nltk.download('movie_reviews') # Data for Naive Bayes training\n",
    "unhappy_text = TextBlob(\"But, I don't like how I can't really tell how it makes its decisions.\", analyzer=NaiveBayesAnalyzer())\n",
    "unhappy_text.sentiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lMGwhy0zFxTz",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "If you filter to get just strong polarity and subjectivity, you may have better luck than trying to classify subtle sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7NThjwzBF-n8",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "unhappy_text = TextBlob(\"I hate this stupid thing!\")\n",
    "unhappy_text.sentiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hsnTi-GDG9-8",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Cleaning for machine learning\n",
    "\n",
    "If you're planning on using some text for some machine learning, you may want to standardize the text in some ways first.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "\n",
    "If your algorithm counts \"swim\" and \"swimming\" as two totally different things, you may lose out on an algorithmic realization that both passages were talking about the same thing.  However, being too aggressive, and doing something like deleting \"ing\" from the ends of all words, could result in genuinely different words being treated as the same.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "\n",
    "In general, this kind of fixing matters more for small texts than very large ones; a very big machine learning corpus could have enough mentions of swim, swims, swimming, and swam that the learner can realize their connection on its own.  The big algorithm then can benefit from using the subtle distinctions between the words.  But for many more humble projects, it can give the algorithm a boost to \"normalize\" the words.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "\n",
    "TextBlobs offer both \"stemming\" and \"lemmatizing.\"  \"Stemming\" is a less nuanced approach that returns a piece of a word with prefixes and suffixes removed that may not be a real word.  \"Lemmatizing\" factors in nearby words and returns a real word, but it can be conservative in TextBlobs and tends to leave words alone.  Neither is likely to be perfect, but either can aid machine learning when the training data is small."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Jc9-yXNPJkl3",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "text = TextBlob(\"I am enjoying these delicious strawberries\")\n",
    "text.words.stem()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8KoqGk_WJ_TH",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "nltk.download('wordnet')\n",
    "text.words.lemmatize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XeouONkEKgOh",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "To aid machine learning, you may want to disregard some \"filler\" words entirely, such as \"a\" and \"the.\"  These are called \"stop words,\" and nltk provides lists for several languages, including English.  Like stemming and lemmatizing, a learner with quite a lot of data might benefit from the subtle information these stop words provide, but for smaller projects, it's probably best to leave them out and avoid confusing the learner with them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "09XbDzifLIVQ",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "nltk.download('stopwords')\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "stops = stopwords.words('english')\n",
    "stops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "445ZerbSLXF4",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "text = TextBlob(\"I was going to say a lot, but maybe not now\")\n",
    "# Notice how you can apply a filter within a list comprehension using \"if\"\n",
    "nonstop = [word for word in text.words if word not in stops]\n",
    "print(nonstop)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4sLLA8KcMKam",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Rather than learning on the basis of individual word frequencies, many machine learning algorithms benefit from seeing 2 or 3 word phrases, and using those frequencies instead of individual word frequencies.  TextBlobs possess an ngrams method that generates the list of n-word phrases for a sentence.  In this way, a word \"the\" that might have been discarded as a stop word could gain new life as part of *The Great Escape*, a famous movie title and \"trigram.\"  (\"Gram\" means \"word.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2ryGT-blMx-6",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "blob = TextBlob(\"Let's go watch 'The Great Escape.'\")\n",
    "blob.ngrams(n = 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# WordNet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another functionality that TextBlobs borrow from NLTK is the ability to access WordNet, a database that contains definitions and lists of synonyms for each word.  This can be used as a stepping stone for more machine learning, although recently machine learning has steered towards large amounts of unstructured data over structured data like this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "nltk.download('wordnet')\n",
    "\n",
    "from textblob import Word\n",
    "\n",
    "net = Word('net')\n",
    "\n",
    "net.definitions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "A \"synset\" is a set of synonyms, indexed by a word, the part of speech of the word, and the definition number of the word that is being referred to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "net.synsets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "We can get words back out by iterating through synset.lemmas(), each of which is a lemmatized synonym that falls under that synset's definition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "syns = set()\n",
    "\n",
    "for word in net.synsets[0].lemmas(): # Just the Internet category of synonyms\n",
    "    syns.add(word.name())\n",
    "print(syns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V7d49hQ_RAk8",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Top words\n",
    "\n",
    "Here's an example from our textbook that plots the frequencies of the top 20 words in *Romeo and Juliet*, not counting stop words.\n",
    "(The text is from [Project Gutenberg](https://www.gutenberg.org), which contains simple text versions of many classics.)  Visualization like this can raise interesting questions for further exploration.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "\n",
    "First, we need to read in the text from a file.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Google colab specific upload\n",
    "from google.colab import files\n",
    "\n",
    "uploaded = files.upload()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0W4qSTN-R2Cg",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "with open('RomeoAndJuliet.txt', 'r') as myfile:\n",
    "    text = myfile.read() # reads all of it\n",
    "    blob = TextBlob(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fPiOouFOTz_V",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "An additional feature of a blob is that it contains a dictionary from words to counts - albeit one that isn't stemmed or lemmatized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SjDrI1sSUAfk",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "print(blob.word_counts['thy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W1Fs5C4YUPj7",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "We can get (word, count) pairs using blob.word_counts.items(), and drop tuples with words that are also stopwords."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MI4GPOP-UeWz",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "items = [item for item in blob.word_counts.items() if item[0] not in stops]\n",
    "print(items[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YweFEjJrUyTq",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "We'll now sort the tuples that didn't contain stopwords, from highest count to smallest count.  sorted is a built-in function for sorting, and the key keyword takes a function to apply to each item before trying to sort- so we'll use a lambda to grab the wordcount part of the tuple."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FJ1xDhesVUur",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "sorted_items= sorted(items, key=lambda x: x[1], reverse=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I2pQy90EVtXv",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "We'll then use pandas to do the final plotting, using its built-in bar chart creator.  We'll skip item 0, which happens to be an apostrophe and is therefore a little underwhelming."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZTq4MmQuV6kY",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.DataFrame(sorted_items[1:21],columns=['word','count'])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "axes = df.plot.bar(x='word',y='count',legend=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Wordcloud module\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "!pip install wordcloud\n",
    "\n",
    "with open('RomeoAndJuliet.txt', 'r') as myfile:\n",
    "    text = myfile.read() # reads all of it\n",
    "\n",
    "from wordcloud import WordCloud\n",
    "wordcloud = WordCloud(background_color = 'white', height=800, width=800)\n",
    "wordcloud = wordcloud.generate(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.imshow(wordcloud)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "wordcloud = wordcloud.to_file('RandJWordCloud.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Google colab - \"how do we get it down from there?\"\n",
    "files.download('RandJWordCloud.png')"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "colab": {
   "name": "Lecture22NLPwithTextBlobs.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "rise": {
   "scroll": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
