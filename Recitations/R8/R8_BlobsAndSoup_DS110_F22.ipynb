{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "R8_BlobsAndSoup_DS110_F22.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "0. First, install TextBlob and download the 'punkt' data if you need to."
      ],
      "metadata": {
        "id": "i2IBVaYPxN-B"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CHdUgF9jskOj"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "nltk.download('punkt')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install textblob\n"
      ],
      "metadata": {
        "id": "_WW4aEE4w6jk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from textblob import TextBlob"
      ],
      "metadata": {
        "id": "GWybgSE5w-VX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.  Use TextBlob to find the individual words in the sentence, \"If I had $1,000,000.00, I'd be rich.\"\n"
      ],
      "metadata": {
        "id": "K30AZNjbwwjW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO"
      ],
      "metadata": {
        "id": "gLv0OBynxXql"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "2.  Download the speech tagger, then find and interpret the tag that is given to the million dollars in the sentence above."
      ],
      "metadata": {
        "id": "Q9eHmLAix6P5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('averaged_perceptron_tagger')"
      ],
      "metadata": {
        "id": "f2bPMx6eyZRv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO get tags"
      ],
      "metadata": {
        "id": "O1N6N8JDybhX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('tagsets')\n",
        "nltk.help.upenn_tagset()"
      ],
      "metadata": {
        "id": "dA1-RhgQyz8F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "3.  Try using the default TextBlob sentiment analyzer on the strings, \"Woo hoo!  I'm rich!\" and \"\"Too bad I'm not actually rich.\""
      ],
      "metadata": {
        "id": "xkXPOeMH0KIs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO sentiment analysis"
      ],
      "metadata": {
        "id": "j_kMsSSc0X_L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "4.  Try removing the stopwords from the sentence, \"If I can't have a million dollars, nobody else should have a million dollars, either.\""
      ],
      "metadata": {
        "id": "ag7ykjGF2c_m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import stopwords\n",
        "nltk.download('stopwords')\n",
        "# TODO"
      ],
      "metadata": {
        "id": "B8M1yidn2mgl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "5.  Now write a function that iterates through the TextBlob-discovered sentences in a string, and for each, ignore it if it's missing an NNP (singular proper noun).  If it has at least one NNP, create a tuple out of the NNP word that was found and the sentiment polarity of the sentence.  Return a list of these tuples.  For example, analysis of the string \"I hate Coke Products.  But I love Honest Tea.\"  might return [(\"Coke\",-0.8),(\"Honest\",0.55), (\"Tea\",0.55)]."
      ],
      "metadata": {
        "id": "cSF26avi3wAl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def nnp_sentiment(string):\n",
        "  # TODO\n",
        "\n",
        "print(nnp_sentiment(\"I hate Coke products.  But I love Honest Tea.\"))"
      ],
      "metadata": {
        "id": "bf0zv6bf4-8Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "6. Try using the requests module and BeautifulSoup to grab all the large headers (h1 and h2) from this text of Alice's Adventures in Wonderland: https://www.gutenberg.org/files/11/11-h/11-h.htm\n",
        "\n",
        "Store the header texts as a list, then print the list.\n"
      ],
      "metadata": {
        "id": "kZzvmwkzxrWv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO"
      ],
      "metadata": {
        "id": "3uoMeJ3nxyev"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}